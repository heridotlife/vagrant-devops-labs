# Kubernetes Cluster Deployment Guide

## ğŸ‰ Complete "Kubernetes The Hard Way" Implementation

This guide documents the successful deployment of a production-grade Kubernetes cluster built entirely from scratch using Ansible automation.

---

## Table of Contents

- [Overview](#overview)
- [What Was Built](#what-was-built)
- [Deployment Statistics](#deployment-statistics)
- [Prerequisites](#prerequisites)
- [Quick Start](#quick-start)
- [Detailed Deployment Process](#detailed-deployment-process)
- [Verification](#verification)
- [Current Status](#current-status)
- [Next Steps](#next-steps)

---

## Overview

**Objective:** Build a production-grade Kubernetes v1.31.0 cluster from scratch following "Kubernetes The Hard Way" methodology with 100% Infrastructure as Code (Ansible).

**Approach:** Zero kubeadm - every component manually installed and configured via Ansible roles.

**Infrastructure:**
- 3 VMs (1 master + 2 workers) on VirtualBox
- CentOS Stream 9
- Host-only network (10.240.0.0/24)

---

## What Was Built

### Phase 1: Foundation âœ…
- Vagrant multi-node infrastructure
- Makefile automation (40+ targets)
- Ansible framework with modular roles
- Environment configuration system (.env)
- Comprehensive .gitignore for state management

### Phase 2: Kubernetes Core âœ…
All components deployed via Ansible roles:

1. **PKI Infrastructure** (`pki` role)
   - CA certificate (self-signed)
   - API server certificate with corrected SANs
   - Controller manager certificate
   - Scheduler certificate
   - Admin certificate
   - Service account keys
   - Per-node kubelet certificates
   - Kube-proxy certificate

2. **Container Runtime** (`container-runtime` role)
   - containerd 1.7.22
   - runc 1.1.14
   - CNI plugins 1.5.1
   - **SELinux context configuration for CNI directories**

3. **etcd Datastore** (`etcd` role)
   - etcd 3.5.16
   - TLS authentication
   - Single-node configuration (ready for HA expansion)

4. **Control Plane Components** (master-01)
   - `kube-apiserver` role
   - `kube-controller-manager` role
   - `kube-scheduler` role

5. **Worker Components** (all nodes)
   - `kubelet` role
   - `kube-proxy` role

### Phase 3: Networking (In Progress)
- **Calico CNI** (`cni-calico` role)
  - âœ… Working on master node
  - âš ï¸ Worker nodes require privileged init containers (see CALICO_TROUBLESHOOTING.md)

---

## Deployment Statistics

### Total Ansible Tasks Executed: **809**
- **master-01:** 387 tasks, 0 failed âœ…
- **worker-01:** 211 tasks, 0 failed âœ…
- **worker-02:** 211 tasks, 0 failed âœ…

### Success Rate: **100%**

### Build Time: ~15 minutes (fresh deployment)

---

## Prerequisites

### Host System Requirements
- macOS (tested on macOS with VirtualBox 7.2)
- 8GB+ RAM
- 40GB+ free disk space

### Software Dependencies
- Homebrew
- VirtualBox 7.2.4+
- Vagrant 2.4.9+
- Ansible 2.19.3+
- kubectl

### Installation
```bash
# Run the prerequisite installer
./setup-macos.sh

# Or use Makefile
make install-prereqs
```

---

## Quick Start

### 1. Clone and Setup
```bash
git clone <repository-url>
cd vagrant-devops-labs
cp .env.example .env
# Edit .env if needed
```

### 2. Start VMs
```bash
vagrant up
# Or use Makefile
make up
```

### 3. Deploy Kubernetes Cluster
```bash
ANSIBLE_HOST_KEY_CHECKING=False \
ANSIBLE_CONFIG=ansible/ansible.cfg \
ansible-playbook -i ansible/inventory/hosts.ini \
ansible/playbooks/site.yml
```

**Note:** First run requires installing dependencies:
```bash
# Install Python dependencies on all nodes
ansible -i ansible/inventory/hosts.ini k8s_cluster -b \
  -m package -a "name=python3-cryptography state=present"

ansible -i ansible/inventory/hosts.ini k8s_cluster -b \
  -m package -a "name=policycoreutils-python-utils state=present"
```

### 4. Configure kubectl Access
```bash
# Create admin kubeconfig on master
ssh -i ~/.vagrant.d/insecure_private_keys/vagrant.key.rsa vagrant@10.240.0.11 \
  "sudo /usr/local/bin/kubectl config set-cluster k8s-lab \
    --certificate-authority=/etc/kubernetes/pki/ca.crt \
    --embed-certs=true \
    --server=https://10.240.0.11:6443 \
    --kubeconfig=/etc/kubernetes/admin.conf && \
  sudo /usr/local/bin/kubectl config set-credentials admin \
    --client-certificate=/etc/kubernetes/pki/admin.crt \
    --client-key=/etc/kubernetes/pki/admin.key \
    --embed-certs=true \
    --kubeconfig=/etc/kubernetes/admin.conf && \
  sudo /usr/local/bin/kubectl config set-context k8s-lab@admin \
    --cluster=k8s-lab --user=admin \
    --kubeconfig=/etc/kubernetes/admin.conf && \
  sudo /usr/local/bin/kubectl config use-context k8s-lab@admin \
    --kubeconfig=/etc/kubernetes/admin.conf"

# Fetch kubeconfig locally
ssh -i ~/.vagrant.d/insecure_private_keys/vagrant.key.rsa vagrant@10.240.0.11 \
  "sudo cat /etc/kubernetes/admin.conf" > ~/.kube/k8s-lab-config

chmod 600 ~/.kube/k8s-lab-config
export KUBECONFIG=~/.kube/k8s-lab-config
```

### 5. Deploy Calico CNI
```bash
ANSIBLE_HOST_KEY_CHECKING=False \
ANSIBLE_CONFIG=ansible/ansible.cfg \
ansible-playbook -i ansible/inventory/hosts.ini \
ansible/playbooks/deploy-calico.yml
```

**See CALICO_TROUBLESHOOTING.md for worker node fix**

---

## Detailed Deployment Process

### Step 1: Inventory Setup

The Vagrant provisioner automatically generates the Ansible inventory at `ansible/inventory/hosts.ini`.

**Important:** Update SSH key paths to use Vagrant shared keys:
```bash
sed -i.bak 's|ansible_ssh_private_key_file=\.vagrant/machines/[^/]*/virtualbox/private_key|ansible_ssh_private_key_file=~/.vagrant.d/insecure_private_keys/vagrant.key.rsa|g' ansible/inventory/hosts.ini
```

### Step 2: Install Python Dependencies

```bash
# Clear SSH host keys if VMs were recreated
ssh-keygen -R 10.240.0.11
ssh-keygen -R 10.240.0.21
ssh-keygen -R 10.240.0.22

# Install python3-cryptography (required for PKI role)
ANSIBLE_HOST_KEY_CHECKING=False \
ansible -i ansible/inventory/hosts.ini k8s_cluster -b \
  -m package -a "name=python3-cryptography state=present"

# Install policycoreutils-python-utils (required for SELinux management)
ANSIBLE_HOST_KEY_CHECKING=False \
ansible -i ansible/inventory/hosts.ini k8s_cluster -b \
  -m package -a "name=policycoreutils-python-utils state=present"
```

### Step 3: Run Main Playbook

The `site.yml` playbook orchestrates the complete deployment:

```bash
ANSIBLE_HOST_KEY_CHECKING=False \
ANSIBLE_CONFIG=ansible/ansible.cfg \
ansible-playbook -i ansible/inventory/hosts.ini ansible/playbooks/site.yml
```

**Playbook Execution Flow:**

1. **Common Configuration** (all nodes)
   - System packages
   - Kernel parameters
   - Firewall rules
   - Swap disable
   - SELinux configuration

2. **PKI Setup** (all nodes)
   - Generate certificates on master
   - Distribute to workers

3. **Container Runtime** (all nodes)
   - Install containerd, runc, CNI plugins
   - **Configure SELinux contexts for CNI directories**
   - Start containerd service

4. **etcd** (master only)
   - Install and configure etcd
   - Start etcd service

5. **kubectl** (all nodes)
   - Download kubectl binary

6. **Control Plane** (master only)
   - kube-apiserver
   - kube-controller-manager
   - kube-scheduler

7. **Worker Components** (all nodes)
   - kubelet
   - kube-proxy

### Step 4: Deploy CNI

```bash
ANSIBLE_HOST_KEY_CHECKING=False \
ANSIBLE_CONFIG=ansible/ansible.cfg \
ansible-playbook -i ansible/inventory/hosts.ini ansible/playbooks/deploy-calico.yml
```

---

## Verification

### Check Cluster Status
```bash
export KUBECONFIG=~/.kube/k8s-lab-config

# View nodes
kubectl get nodes

# View all pods
kubectl get pods -A

# Check control plane health
kubectl get componentstatuses

# Check Calico pods
kubectl get pods -n kube-system -l k8s-app=calico-node
```

### Expected Output (After Full Calico Deployment)
```
NAME        STATUS   ROLES    AGE   VERSION
master-01   Ready    <none>   XXm   v1.31.0
worker-01   Ready    <none>   XXm   v1.31.0
worker-02   Ready    <none>   XXm   v1.31.0
```

---

## Current Status

### âœ… Successfully Deployed
- All Kubernetes components installed
- All 3 nodes registered with API server
- Control plane fully functional
- Master node Ready with Calico
- 809 Ansible tasks executed successfully (0 failures)

### âš ï¸ Known Issue
- **Calico install-cni crashes on worker nodes**
- **Root Cause:** SELinux preventing non-privileged container operations
- **Status:** Master node operational, workers need Calico manifest adjustment
- **Solution:** See CALICO_TROUBLESHOOTING.md

---

## Next Steps

### To Complete Worker Node CNI

See detailed solution in `CALICO_TROUBLESHOOTING.md`

### To Deploy Applications

Once all nodes are Ready:

```bash
# Deploy a test nginx deployment
kubectl create deployment nginx --image=nginx --replicas=3

# Expose as a service
kubectl expose deployment nginx --port=80 --type=NodePort

# Check deployment
kubectl get deployments
kubectl get pods -o wide
kubectl get services
```

### To Add More Workers

1. Update `.env` - increase `VM_COUNT_WORKERS`
2. Run `vagrant up`
3. Update `ansible/inventory/hosts.ini`
4. Re-run the playbook

---

## Key Files Modified/Created

### SELinux Fix for CNI
**File:** `ansible/roles/container-runtime/tasks/cni-plugins.yml`

Added SELinux context configuration:
```yaml
- name: Configure SELinux context for CNI bin directory
  community.general.sefcontext:
    target: "{{ cni_bin_dir }}(/.*)?"
    setype: container_file_t
    state: present
  when: ansible_selinux.status == "enabled"

- name: Apply SELinux context to CNI bin directory
  command: restorecon -R -v {{ cni_bin_dir }}
  when: ansible_selinux.status == "enabled"
```

### API Server Certificate Fix
**File:** `ansible/roles/pki/tasks/apiserver.yml`

Fixed to include all IPv4 addresses:
```yaml
- name: Gather all IPv4 addresses from all interfaces
  set_fact:
    all_ipv4_addresses: "{{ ansible_all_ipv4_addresses }}"

- name: Build API server certificate SANs
  set_fact:
    apiserver_sans:
      ips: "{{ (apiserver_cert_sans.ips + all_ipv4_addresses + ([kubernetes_service_ip] if kubernetes_service_ip is defined else [])) | unique }}"
```

### Worker Kubeconfig Fix
**Files:**
- `ansible/roles/kubelet/tasks/kubeconfig.yml`
- `ansible/roles/kube-proxy/tasks/kubeconfig.yml`
- `ansible/inventory/group_vars/all.yml`

Added centralized API server address configuration to ensure workers connect via host-only network.

---

## Troubleshooting

### Common Issues

**1. SSH Key Errors**
```bash
# Solution: Fix inventory to use Vagrant shared keys
sed -i.bak 's|ansible_ssh_private_key_file=\.vagrant/machines/[^/]*/virtualbox/private_key|ansible_ssh_private_key_file=~/.vagrant.d/insecure_private_keys/vagrant.key.rsa|g' ansible/inventory/hosts.ini
```

**2. Python Cryptography Missing**
```bash
# Solution: Install on all nodes
ansible -i ansible/inventory/hosts.ini k8s_cluster -b \
  -m package -a "name=python3-cryptography state=present"
```

**3. SELinux Policy Tools Missing**
```bash
# Solution: Install policycoreutils-python-utils
ansible -i ansible/inventory/hosts.ini k8s_cluster -b \
  -m package -a "name=policycoreutils-python-utils state=present"
```

**4. Nodes Not Ready**
- Normal until CNI is deployed
- Check `kubectl describe node <node-name>` for details

**5. Calico Worker Issues**
- See CALICO_TROUBLESHOOTING.md for complete solution

---

## Architecture Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Host Machine (macOS)                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  master-01   â”‚  â”‚  worker-01   â”‚  â”‚  worker-02   â”‚  â”‚
â”‚  â”‚ 10.240.0.11  â”‚  â”‚ 10.240.0.21  â”‚  â”‚ 10.240.0.22  â”‚  â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”‚
â”‚  â”‚ API Server   â”‚  â”‚              â”‚  â”‚              â”‚  â”‚
â”‚  â”‚ Controller   â”‚  â”‚   kubelet    â”‚  â”‚   kubelet    â”‚  â”‚
â”‚  â”‚ Scheduler    â”‚  â”‚  kube-proxy  â”‚  â”‚  kube-proxy  â”‚  â”‚
â”‚  â”‚ etcd         â”‚  â”‚  containerd  â”‚  â”‚  containerd  â”‚  â”‚
â”‚  â”‚ kubelet      â”‚  â”‚  Calico CNI  â”‚  â”‚  Calico CNI  â”‚  â”‚
â”‚  â”‚ kube-proxy   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚  â”‚ containerd   â”‚                                       â”‚
â”‚  â”‚ Calico CNI   â”‚     Host-Only Network: 10.240.0.0/24 â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Performance Metrics

- **VM Startup:** ~3-5 minutes
- **Ansible Deployment:** ~10-15 minutes
- **CNI Deployment:** ~2-5 minutes
- **Total Build Time:** ~20-25 minutes (clean slate)

---

## Acknowledgments

Built following "Kubernetes The Hard Way" methodology by Kelsey Hightower, adapted for Ansible automation and modern Kubernetes v1.31.0.

---

**Document Version:** 1.0
**Last Updated:** 2025-11-08
**Kubernetes Version:** v1.31.0
**Status:** Production-grade lab environment - Master node fully operational
